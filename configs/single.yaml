defaults:
  - model: nlb_mc_rtt
  - datamodule: nlb_mc_rtt
  - callbacks: default
  - _self_
seed: 0
ignore_warnings: False

trainer:
  _target_: pytorch_lightning.Trainer
  gradient_clip_val: 200
  max_epochs: 1_000
  log_every_n_steps: 5
  # track_grad_norm: 2

callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    monitor: valid/recon_smth
    mode: min
    save_top_k: 1
    save_last: True
    verbose: False
    dirpath: lightning_checkpoints
    auto_insert_metric_name: False
  early_stopping:
    _target_: lfads_torch.extensions.tune.EarlyStoppingWithBurnInPeriod
    monitor: valid/recon_smth
    mode: min
    patience: 200
    min_delta: 0
    burn_in_period: ${max:${sum:${model.l2_start_epoch},${model.l2_increase_epoch}},${sum:${model.kl_start_epoch},${model.kl_increase_epoch}}}
  learning_rate_monitor:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: epoch

logger:
  csv_logger:
    _target_: pytorch_lightning.loggers.CSVLogger
    save_dir: "csv_logs"
    version: ""
    name: ""
  tensorboard_logger:
    _target_: pytorch_lightning.loggers.TensorBoardLogger
    save_dir: "."
    version: ""
    name: ""

posterior_sampling:
  use_best_ckpt: True
  fn:
    _target_: lfads_torch.post_run.analysis.run_posterior_sampling
    filename: lfads_output.h5
    num_samples: 50
